#\!/usr/bin/env python3
"""
Boss1å ±å‘Šãƒ­ã‚°è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 
President0ã«ã‚ˆã‚Šè¨­è¨ˆãƒ»å®Ÿè£…
"""

import os
import shutil
import datetime
import re
from pathlib import Path

class LogCleanupManager:
    def __init__(self, base_dir="/mnt/c/home/hiroshi/blog_generator"):
        self.base_dir = Path(base_dir)
        self.message_queue_dir = self.base_dir / "tmp" / "message_queue"
        self.backup_dir = self.base_dir / "tmp" / "log_backups"
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è¨­å®š
        self.max_log_size_kb = 20  # 20KBåˆ¶é™
        self.max_entries_per_log = 100  # æœ€æ–°100ã‚¨ãƒ³ãƒˆãƒªã®ã¿ä¿æŒ
        self.retention_hours = 24  # 24æ™‚é–“ä»¥ä¸Šå¤ã„ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def get_log_files(self):
        """å¯¾è±¡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—"""
        log_files = []
        if self.message_queue_dir.exists():
            for log_file in self.message_queue_dir.glob("*_queue.log"):
                log_files.append(log_file)
        return log_files
    
    def parse_log_entry_timestamp(self, line):
        """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è§£æ"""
        timestamp_pattern = r"(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})"
        match = re.match(timestamp_pattern, line)
        if match:
            try:
                return datetime.datetime.strptime(match.group(1), "%Y-%m-%d %H:%M:%S")
            except ValueError:
                return None
        return None
    
    def backup_log_file(self, log_file):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"{log_file.stem}_backup_{timestamp}.log"
        backup_path = self.backup_dir / backup_filename
        
        shutil.copy2(log_file, backup_path)
        print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ: {backup_path}")
        return backup_path
    
    def cleanup_old_entries_by_time(self, log_file):
        """æ™‚é–“ãƒ™ãƒ¼ã‚¹ã§ã®å¤ã„ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤"""
        if not log_file.exists():
            return
        
        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=self.retention_hours)
        kept_lines = []
        
        with open(log_file, 'r', encoding='utf-8') as f:
            for line in f:
                entry_time = self.parse_log_entry_timestamp(line.strip())
                if entry_time and entry_time >= cutoff_time:
                    kept_lines.append(line)
                elif not entry_time:  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—ã®è¡Œã¯ä¿æŒ
                    kept_lines.append(line)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãæ›ãˆ
        with open(log_file, 'w', encoding='utf-8') as f:
            f.writelines(kept_lines)
        
        print(f"âœ… æ™‚é–“ãƒ™ãƒ¼ã‚¹å‰Šé™¤å®Œäº†: {log_file.name} ({len(kept_lines)}è¡Œä¿æŒ)")
    
    def cleanup_excess_entries(self, log_file):
        """ã‚¨ãƒ³ãƒˆãƒªæ•°åˆ¶é™ã§ã®å‰Šé™¤"""
        if not log_file.exists():
            return
        
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        if len(lines) > self.max_entries_per_log:
            # æœ€æ–°ã®ã‚¨ãƒ³ãƒˆãƒªã®ã¿ä¿æŒ
            kept_lines = lines[-self.max_entries_per_log:]
            
            with open(log_file, 'w', encoding='utf-8') as f:
                f.writelines(kept_lines)
            
            print(f"âœ… ã‚¨ãƒ³ãƒˆãƒªæ•°åˆ¶é™å‰Šé™¤å®Œäº†: {log_file.name} ({len(kept_lines)}è¡Œä¿æŒ)")
    
    def cleanup_by_file_size(self, log_file):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã§ã®å‰Šé™¤"""
        if not log_file.exists():
            return
        
        file_size_kb = log_file.stat().st_size / 1024
        
        if file_size_kb > self.max_log_size_kb:
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ä»¥ä¸‹ã«ãªã‚‹ã¾ã§å¤ã„ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤
            while len(lines) > 10:  # æœ€ä½10è¡Œã¯ä¿æŒ
                lines = lines[10:]  # 10è¡Œãšã¤å‰Šé™¤
                
                temp_content = ''.join(lines)
                if len(temp_content.encode('utf-8')) / 1024 <= self.max_log_size_kb:
                    break
            
            with open(log_file, 'w', encoding='utf-8') as f:
                f.writelines(lines)
            
            print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™å‰Šé™¤å®Œäº†: {log_file.name} ({len(lines)}è¡Œä¿æŒ)")
    
    def cleanup_old_backups(self):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆ7æ—¥ä»¥ä¸Šå¤ã„ï¼‰"""
        if not self.backup_dir.exists():
            return
        
        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=7)
        deleted_count = 0
        
        for backup_file in self.backup_dir.glob("*_backup_*.log"):
            if backup_file.stat().st_mtime < cutoff_time.timestamp():
                backup_file.unlink()
                deleted_count += 1
        
        if deleted_count > 0:
            print(f"âœ… å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤: {deleted_count}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    def execute_full_cleanup(self):
        """å®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        print("ğŸ§¹ Boss1å ±å‘Šãƒ­ã‚°è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")
        print(f"ğŸ“ å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.message_queue_dir}")
        
        log_files = self.get_log_files()
        
        if not log_files:
            print("âš ï¸ å¯¾è±¡ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        for log_file in log_files:
            print(f"\nğŸ“‹ å‡¦ç†ä¸­: {log_file.name}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
            file_size_kb = log_file.stat().st_size / 1024
            print(f"   ç¾åœ¨ã‚µã‚¤ã‚º: {file_size_kb:.1f}KB")
            
            # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            if file_size_kb > self.max_log_size_kb:
                self.backup_log_file(log_file)
                
                # æ®µéšçš„ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
                self.cleanup_old_entries_by_time(log_file)
                self.cleanup_excess_entries(log_file)
                self.cleanup_by_file_size(log_file)
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®ã‚µã‚¤ã‚ºç¢ºèª
                new_size_kb = log_file.stat().st_size / 1024
                print(f"   ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œ: {new_size_kb:.1f}KB")
            else:
                print(f"   âœ… ã‚µã‚¤ã‚ºé©æ­£ï¼ˆ{file_size_kb:.1f}KB â‰¤ {self.max_log_size_kb}KBï¼‰")
        
        # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        self.cleanup_old_backups()
        
        print("\nâœ… Boss1å ±å‘Šãƒ­ã‚°ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœã‚µãƒãƒªãƒ¼
        self.print_cleanup_summary()
    
    def print_cleanup_summary(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
        print("\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        
        log_files = self.get_log_files()
        total_size = 0
        
        for log_file in log_files:
            file_size_kb = log_file.stat().st_size / 1024
            total_size += file_size_kb
            print(f"  {log_file.name}: {file_size_kb:.1f}KB")
        
        print(f"  åˆè¨ˆã‚µã‚¤ã‚º: {total_size:.1f}KB")
        print(f"  ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¿å­˜å…ˆ: {self.backup_dir}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    cleanup_manager = LogCleanupManager()
    cleanup_manager.execute_full_cleanup()

if __name__ == "__main__":
    main()
EOF < /dev/null
